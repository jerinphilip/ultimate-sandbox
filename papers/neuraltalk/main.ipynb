{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NeuralTalk\n",
    "\n",
    "* Deep Semantic Visual Embeddings for Image Captioning - Andrej Karpathy, Fei Fei Li.\n",
    "\n",
    "The following notebook glues together the components, which are defined across several files in the folder to a linear story which reasonably reproduces the results of the paper.\n",
    "\n",
    "I'm not going to be particular about the details - for example, I'll use and LSTM instead of an RNN, put in Linear layers in between for changing sizes to experiment how the number of parameters affect the results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python3\n",
    "from argparse import ArgumentParser\n",
    "import json\n",
    "from pprint import pprint\n",
    "from itertools import chain\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import matplotlib\n",
    "#matplotlib.use('Agg')\n",
    "from matplotlib import pyplot as plt\n",
    "import os,sys\n",
    "import string\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import datetime\n",
    "from tensorboardX import SummaryWriter\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "The tokens are already available preprocessed in the `.json` file, we're going to reuse them. Thus we require only parsing. \n",
    "\n",
    "A Flickr DataLoader is adapted from [here](https://github.com/fartashf/vsepp/blob/master/data.py). It supports batching together variable length targets with padding through a collate function, which is the general practice in pytorch.\n",
    "\n",
    "The Dataset class requires an additional `Vocab` object which is function which returns a unique index corresponding to a unique token. We'll code our own up and keep two things in mind:\n",
    "1. `vocab(token)` gives id, no error - for unknown, give a token corresponding to unknown.\n",
    "2. `len(vocab)` gives the total number of words in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "uspath = '/home/jerin/code/ultimate-sandbox'\n",
    "flickr_root = '/tmp/Flickr-8K/Flicker8k_Dataset'\n",
    "sys.path.insert(0, uspath)\n",
    "from preproc import Vocab, img_preprocess, extract_tokens\n",
    "from usandbox.data import FlickrDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1919"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Construct vocabulary\n",
    "json_file = 'dataset_flickr8k.json'\n",
    "tokens = extract_tokens(json_file)\n",
    "vocab = Vocab(tokens)\n",
    "vocab('you')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(name):\n",
    "    return FlickrDataset(flickr_root, json_file, name, \n",
    "                        vocab, transform=img_preprocess)\n",
    "\n",
    "dataset = {}\n",
    "for phase in ['train', 'test', 'val']:\n",
    "    dataset[phase] = get_dataset(phase)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              0   0%    0.00kB/s    0:00:00 (xfr#0, to-chk=0/16197)   \n"
     ]
    }
   ],
   "source": [
    "!rsync -rz --info=progress2 ada:/share1/dataset/Flickr-8K/ /tmp/Flickr-8K/ --append"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_params = {\n",
    "    \"batch_size\": 512,\n",
    "    \"shuffle\"  : True,\n",
    "    \"num_workers\" : 20,\n",
    "    \"collate_fn\": FlickrDataset.collate_fn\n",
    "}\n",
    "\n",
    "def n_batches(dataset):\n",
    "    return len(dataset)//batch_params[\"batch_size\"] + 1\n",
    "\n",
    "def loader(dataset):\n",
    "    return torch.utils.data.DataLoader(dataset=dataset, **batch_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models\n",
    "\n",
    "Models which are used in composition are defined in `models.py`. Some are tiny enough to be declared at a later stage, from torch's own predefined building blocks. The models are constituted by:\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. CNN Feature Extractor\n",
    "A modified `Resnet18`, which I call `ResnetMinus`, since it lacks the last softmax layer thereby giving be dense features representing the image is used to supply the hidden representation for a generative RNN model, which predicts the captions. \n",
    "\n",
    "Learning how to use the pretrained available `Resnet18` and dropping layers, freezing the parameters so that the gradients are not computed while training for captioning were first time hands on learning for me. Turns out pytorch only requires you to iterate through `model.parameters()` and turn `requires_grad=False` for them, so they're frozen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import models\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, embedding):\n",
    "        super().__init__()\n",
    "        self.embed = embedding\n",
    "        self.resnet = models.ResnetMinus()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        r = self.resnet(self.embed(x))\n",
    "        B, H, _, _ = r.size()\n",
    "        r = r.reshape(B, H)\n",
    "        return r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Generative RNN\n",
    "We train an RNN (I use an LSTM variant here) to take in the hidden representation as the image feature given by `ResnetMinus`. The input at the first time step is an index `<start>` token. An additional `nn.Embedding` layer is used to embed the words given by the indices in a dense space. Let's say the caption we are to learn is `z[1:t]` at each time step. \n",
    "\n",
    "The sequence we're trying to learn will be: `z[1:(t-1)] -> z[2:t]`.\n",
    "\n",
    "The values are passed through an output softmax layer and we could do a `Greedy` or `Beam-Search` based decoding to get the captions. We'll stick to `Greedy` in this particular attempt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, params, embedding, interpretor, n_classes):\n",
    "        super().__init__()\n",
    "        self.params = params\n",
    "        self.lstm = nn.LSTM(**params)\n",
    "        self.embed = embedding\n",
    "        self.interpreter = interpreter\n",
    "        self.generator = nn.Linear(params[\"hidden_size\"], n_classes)\n",
    "        \n",
    "    def forward(self, context, seed, teacher_forcing=True):\n",
    "        h = context.repeat(self.params[\"num_layers\"], 1, 1)\n",
    "        c = torch.zeros_like(h)\n",
    "        tgt = self.embed(seed)\n",
    "        y_prev = seed[:, 0:1]\n",
    "        B, T, H = tgt.size()\n",
    "        max_length = T\n",
    "        ys = []\n",
    "        for t in range(max_length-1):\n",
    "            yt = tgt[:, t:t+1, :] if teacher_forcing else self.embed(y_prev)\n",
    "            y, (h, c) = self.lstm(yt, (h, c))\n",
    "            yt = self.generator(y)\n",
    "            ys.append(yt)\n",
    "            y_prev = self.interpreter.argmax(yt.detach())            \n",
    "        y = torch.cat(ys, dim=1)\n",
    "        return y    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Interpreter:\n",
    "    def __init__(self, vocab):      \n",
    "        self.softmax = nn.Softmax(dim=2)\n",
    "        self.build_vocab(vocab)\n",
    "        \n",
    "    def build_vocab(self, vocab):\n",
    "        self.idx2word = {}\n",
    "        for key, value in vocab.word2idx.items():\n",
    "            self.idx2word[value] = key\n",
    "            \n",
    "    def inverse(self, indices):\n",
    "        tokens = list(map(lambda x: self.idx2word[x], indices))\n",
    "        return tokens\n",
    "    \n",
    "    def argmax(self, acts):\n",
    "        probs = self.softmax(acts)\n",
    "        max_value, max_index = probs.max(dim=2)\n",
    "        return max_index\n",
    "    \n",
    "    def decode(self, acts):\n",
    "        B, T, H = acts.size()\n",
    "        batch = []\n",
    "        indices = self.argmax(acts)\n",
    "        for i in range(B):\n",
    "            tokens = self.inverse(indices[i, :].tolist())\n",
    "            ostr = ' '.join(tokens)\n",
    "            batch.append(ostr)\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TCrossEntropy(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "    def forward(self, y, z):\n",
    "        B, T, H = y.size()\n",
    "        B, T = z.size()\n",
    "        #y = y.permute(1, 0, 2).contiguous()\n",
    "        y = y.view(-1, H)\n",
    "        z = z.contiguous().view(-1)\n",
    "        return self.criterion(y, z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size, hidden_size = 512, 512\n",
    "src_embed = models.Identity()\n",
    "encoder = Encoder(input_size, hidden_size, src_embed)\n",
    "lparams = {\n",
    "    \"input_size\": 100,\n",
    "    \"hidden_size\": 512,\n",
    "    \"num_layers\": 5,\n",
    "    \"dropout\": 0.2,\n",
    "    \"bidirectional\": False,\n",
    "    \"batch_first\": True\n",
    "}\n",
    "\n",
    "tgt_embed = nn.Embedding(len(vocab), lparams[\"input_size\"])\n",
    "interpreter = Interpreter(vocab)\n",
    "decoder = Decoder(lparams, tgt_embed, interpreter, len(vocab))\n",
    "net = models.EncoderDecoder(encoder, decoder)\n",
    "device = torch.device(\"cuda:0\")\n",
    "net = net.to(device)\n",
    "criterion = TCrossEntropy()        \n",
    "optimizer = torch.optim.SGD(filter(lambda p: p.requires_grad, net.parameters()), lr=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from usandbox.stats import Meter    \n",
    "from usandbox.logs import Logger\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, model, loss, optimizer, dataset, run, decoder=None):\n",
    "        self.model = model\n",
    "        self.loss = loss\n",
    "        self.optimizer = optimizer\n",
    "        self.logger = {phase: SummaryWriter(log_dir=\"/tmp/jerin/logs/{}/{}\".format(phase, run)) \n",
    "                       for phase in ['train', 'val']}\n",
    "        self.dataset = dataset\n",
    "        self.best_loss = float(\"inf\")\n",
    "        \n",
    "    def run_epochs(self, max_epochs):\n",
    "        for epoch in tqdm(range(max_epochs), desc='epoch', leave=True):\n",
    "            self.epoch = epoch\n",
    "            self.train()\n",
    "            self.validate()\n",
    "        \n",
    "    def train(self):\n",
    "        self.model.train()\n",
    "        loss = self.process(\"train\")\n",
    "    \n",
    "    def validate(self):\n",
    "        self.model.eval()\n",
    "        loss = self.process(\"val\")\n",
    "        if loss < self.best_loss:\n",
    "            self.best_loss = loss\n",
    "            self.best_model = self.export()\n",
    "    \n",
    "    def export(self):\n",
    "        checkpoint = {\n",
    "            \"model\": self.model.state_dict(),\n",
    "            \"optimizer\": self.optimizer.state_dict()\n",
    "        }\n",
    "        return checkpoint\n",
    "    \n",
    "    def load(self, checkpoint):\n",
    "        self.model.load_state_dict(checkpoint[\"model\"])\n",
    "        self.optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "        \n",
    "    \n",
    "    def clip_grad(self):\n",
    "        max_grad_norm = 5\n",
    "        params = list(filter(lambda x: x.requires_grad, self.model.parameters()))\n",
    "        clip_grad_norm_(params, max_grad_norm)\n",
    "    \n",
    "    def log_decode(self, phase, x, y):\n",
    "        decodes = self.model.decoder.interpreter.decode(y)\n",
    "        def invert(img):\n",
    "            img = img.numpy().transpose((1, 2, 0))\n",
    "            mean = np.array([0.485, 0.456, 0.406])\n",
    "            std = np.array([0.229, 0.224, 0.225])\n",
    "            img = std * img + mean\n",
    "            img = np.clip(img, 0, 1)\n",
    "            return img \n",
    "        \n",
    "        plt.switch_backend('agg')\n",
    "        for i, caption in enumerate(decodes):\n",
    "            for undesirable in [\"<start>\", \"<end>\", \"</start>\"]:\n",
    "                caption = caption.replace(undesirable, \"\")\n",
    "            caption = caption.strip()\n",
    "            plt.clf()\n",
    "            figure = plt.figure(figsize=(10, 10))\n",
    "            plt.title(caption, fontsize=16)\n",
    "            img = x[i, :, :, :].cpu()\n",
    "            img = invert(img)\n",
    "            plt.imshow(img)\n",
    "            plt.axis(\"off\")\n",
    "            trainer.logger[phase].add_figure('captions-{}'.format(self.epoch), figure, i)\n",
    "\n",
    "    \n",
    "    def process(self, phase):\n",
    "        meter = Meter()\n",
    "        dataset = self.dataset[phase]\n",
    "        for i, b in tqdm(enumerate(loader(dataset)), \n",
    "                         total=n_batches(dataset), \n",
    "                         desc=phase, \n",
    "                         leave=False):\n",
    "            if phase == \"train\":\n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "            x, z, *_ = b       \n",
    "            x = x.to(device)\n",
    "            z = z.to(device)\n",
    "            y = net(x, z, teacher_forcing=False)\n",
    "            loss = self.loss(y, z[:, 1:])\n",
    "            meter.report(loss.item())\n",
    "\n",
    "            self.logger[phase].add_scalar('loss', loss.item(), i)\n",
    "            if phase == \"train\":\n",
    "                loss.backward()\n",
    "                self.clip_grad()\n",
    "                self.optimizer.step()    \n",
    "                \n",
    "        self.log_decode(phase, x, y)\n",
    "        self.logger[phase].add_scalar('loss/avg'.format(phase), meter.avg(), self.epoch)\n",
    "        return meter.avg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "trainer = Trainer(net, criterion, optimizer, dataset, run)\n",
    "with open(\"model.weights\", \"rb\") as fp:\n",
    "    checkpoint = torch.load(fp)\n",
    "    trainer.load(checkpoint)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3951f44e496488899cfab1006688950",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='epoch', max=25), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='train', max=59), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='val', max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='train', max=59), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='val', max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='train', max=59), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='val', max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='train', max=59), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='val', max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='train', max=59), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='val', max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='train', max=59), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='val', max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='train', max=59), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='val', max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='train', max=59), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='val', max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='train', max=59), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='val', max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='train', max=59), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='val', max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "736e76d6b61c4ad4a791f843034cad20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='train', max=59), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.run_epochs(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"model.weights\", \"wb+\") as ofp:\n",
    "    torch.save(trainer.export(), ofp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = net(x, z, teacher_forcing=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decodes = interpreter.decode(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for i, caption in enumerate(decodes):\n",
    "    for undesirable in [\"<start>\", \"<end>\", \"</start>\"]:\n",
    "        caption = caption.replace(undesirable, \"\")\n",
    "    caption = caption.strip()\n",
    "    plt.clf()\n",
    "    figure = plt.figure(figsize=(10, 10))\n",
    "    plt.title(caption, fontsize=16)\n",
    "    img = x[i, :, :, :].cpu()\n",
    "    img = invert(img)\n",
    "    plt.imshow(img)\n",
    "    plt.axis(\"off\")\n",
    "    trainer.logger['train'].add_figure('caption-attempt-7', figure, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
